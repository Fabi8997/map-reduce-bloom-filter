import argparse
import mmh3
from pyspark import SparkContext
from typing import Tuple

import bloomfilters_util as util


def parse_arguments() -> Tuple[float, str, str, str]:
    """
    Parse the command line input arguments
    :return: parsed arguments
    """
    parser = argparse.ArgumentParser()
    parser.add_argument('false_positive_prob', type=float, help='probability of false positives')
    parser.add_argument('dataset_input_file', type=str, help='path of the dataset')
    parser.add_argument('linecount_file', type=str, help='path of the linecount output file')
    parser.add_argument('output_file', type=str, help='path of the output file')
    args = parser.parse_args()
    
    return args.false_positive_prob, args.dataset_input_file, args.linecount_file, args.output_file


def generate_bloom_filter(indexes_to_set: list, size: int) -> list:
    """
    Set to True the corresponding item of the Bloom Filter
    
    :param indexes_to_set: list of indexes to set True (computed through hash functions)
    :param size: size of the bloom filter
    
    :return: the Bloom filters structure
    """
    bloom_filter = [False for _ in range(size)]
    
    for i in indexes_to_set:
        bloom_filter[i] = True
    
    return bloom_filter


def extend_list(list1: list, list2: list) -> list:
    """
        Concat the second list to the first one and return the first one

        :param list1: first list
        :param list2: second list

        :return: merged list
    """
    list1.extend(list2)
    return list1


def main():
    false_positive_prob, dataset_input_file, linecount_file, output_file = parse_arguments()
    
    sc = SparkContext(appName="SPARK_BLOOM_FILTERS_BUILDER", master="yarn")
    
    # Add Python dependencies to Spark application
    sc.addPyFile("./bloomfilters_util.py")
    sc.addPyFile(mmh3.__file__)
    
    # Create broadcast variables so that each Spark task can retrieve
    # how many hash functions must be used and the size of each bloom filter
    broadcast_hash_function_number = sc.broadcast(util.compute_number_of_hash_functions(false_positive_prob))
    broadcast_size_of_bloom_filters = sc.broadcast(
        util.get_size_of_bloom_filters(sc, linecount_file, false_positive_prob)
    )
    print()
    print("Number of hash functions: " + str(broadcast_hash_function_number.value))
    print("Size of bloom filters: " + str(broadcast_size_of_bloom_filters.value))
    print()
    
    """
        1. read dataset
        2. map: parse each line of the dataset to extract the rating and a list of indexes to set True in the
                bloom filter (generated by computing the hash values from the movie's id).
                Output the key-value tuple (rating, [indexes]).
        3. reduceByKey: group by rating and merge the `indexes to set` (computed in the previous step) in a single list
                        without duplicates
        4. map: take (rating, [indexes]) and create the bloom filter setting to True the corresponding item of the list
        5. save the results (the bloom filter) as a pickle file
    """
    sc.textFile(dataset_input_file) \
        .map(lambda line:
             util.create_pair_rating_indexes(line,
                                             broadcast_size_of_bloom_filters.value,
                                             broadcast_hash_function_number.value)
             ) \
        .reduceByKey(extend_list) \
        .map(lambda pair_rating_indexes:
                 (pair_rating_indexes[0],
                  generate_bloom_filter(pair_rating_indexes[1],
                                        broadcast_size_of_bloom_filters.value[pair_rating_indexes[0]])
                  )
             ) \
        .saveAsPickleFile(output_file)
    
    print("\n\nBLOOM FILTERS BUILDER COMPLETED\n\n")


if __name__ == '__main__':
    main()
